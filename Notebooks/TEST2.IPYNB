{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab8e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Configuration : Embeddings sur CUDA | Mod√®le : llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Remplacez par votre cl√© API r√©elle\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"Groq_API_KEY\")\n",
    "\n",
    "# Chemins\n",
    "DATA_PATH = \"data/\"            # Mettez vos PDFs dans ce dossier\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "\n",
    "# Mod√®les Optimis√©s\n",
    "# 1. Embeddings MULTILINGUES (Comprend FR <-> EN)\n",
    "MODEL_EMBEDDING = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "# 2. LLM Performant\n",
    "MODEL_LLM = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# D√©tection GPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"‚öôÔ∏è Configuration : Embeddings sur {DEVICE.upper()} | Mod√®le : {MODEL_LLM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "187a92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:90: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:90: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/tmp/ipykernel_15334/899073198.py:90: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  separators=[\"\\n\\n\", \"(?<=\\. )\", \"\\n\", \" \", \"\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ D√©marrage du Traitement Avanc√© (Sur CUDA) ---\n",
      "üìÑ 6866 pages brutes charg√©es.\n",
      "üßπ Nettoyage et Enrichissement des donn√©es...\n",
      "‚úÖ 6796 pages trait√©es et enrichies.\n",
      "‚úÇÔ∏è  G√©n√©ration de 33923 fragments (chunks) optimis√©s.\n",
      "üß† Calcul des vecteurs avec sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15334/899073198.py:98: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base de donn√©es sauvegard√©e avec succ√®s dans 'vectorstore/db_faiss'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# --- CORRECTION DES IMPORTS ---\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# C'est ici que √ßa changeait : on utilise langchain_core maintenant\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "DATA_PATH = \"data/raw\"            # Assurez-vous que vos 5 PDFs sont dans ce dossier\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "\n",
    "# Mod√®le Multilingue (Arabe + Fran√ßais + Anglais)\n",
    "MODEL_EMBEDDING = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fonction de nettoyage avanc√© pour retirer le bruit des PDF.\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "\n",
    "    # 1. Retirer les num√©ros de pages isol√©s\n",
    "    text = re.sub(r'Page\\s+\\d+|^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\d+\\s+\\|\\s+P\\s+a\\s+g\\s+e', '', text) # Cas sp√©cifique Wheeling\n",
    "    \n",
    "    # 2. Retirer les en-t√™tes r√©p√©titifs\n",
    "    text = re.sub(r'Undergraduate\\s+Catalog\\s+2024-2025', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # 3. R√©parer les c√©sures de mots (hyphenation)\n",
    "    text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
    "    \n",
    "    # 4. Supprimer les sauts de ligne multiples\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def load_and_process_documents():\n",
    "    print(f\"--- üöÄ D√©marrage du Traitement Avanc√© (Sur {DEVICE.upper()}) ---\")\n",
    "    \n",
    "    # V√©rification du dossier\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH)\n",
    "        print(f\"‚ö†Ô∏è  Le dossier '{DATA_PATH}' a √©t√© cr√©√©. Veuillez y d√©poser vos PDF et relancer.\")\n",
    "        return\n",
    "\n",
    "    # 1. Chargement Brut\n",
    "    loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    raw_docs = loader.load()\n",
    "    print(f\"üìÑ {len(raw_docs)} pages brutes charg√©es.\")\n",
    "\n",
    "    if not raw_docs:\n",
    "        print(f\"‚ùå Erreur : Le dossier '{DATA_PATH}' est vide. Ajoutez vos PDF.\")\n",
    "        return\n",
    "\n",
    "    processed_docs = []\n",
    "    \n",
    "    # 2. Nettoyage et Injection de Contexte\n",
    "    print(\"üßπ Nettoyage et Enrichissement des donn√©es...\")\n",
    "    for doc in raw_docs:\n",
    "        # Identification de la source\n",
    "        full_source = doc.metadata.get('source', '')\n",
    "        filename = os.path.basename(full_source) # Extrait juste le nom du fichier\n",
    "        uni_name = filename.replace('.pdf', '').replace('_', ' ')\n",
    "        \n",
    "        # Nettoyage\n",
    "        cleaned_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # S'il reste du contenu utile\n",
    "        if len(cleaned_content) > 50:\n",
    "            # INJECTION DE CONTEXTE : On ajoute le nom de l'universit√© au d√©but du chunk\n",
    "            enriched_content = f\"Document Source: {uni_name}\\n\\n{cleaned_content}\"\n",
    "            \n",
    "            # On met √† jour le contenu\n",
    "            doc.page_content = enriched_content\n",
    "            processed_docs.append(doc)\n",
    "\n",
    "    print(f\"‚úÖ {len(processed_docs)} pages trait√©es et enrichies.\")\n",
    "\n",
    "    # 3. Chunking Optimis√©\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200,\n",
    "        chunk_overlap=300,\n",
    "        separators=[\"\\n\\n\", \"(?<=\\. )\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(processed_docs)\n",
    "    print(f\"‚úÇÔ∏è  G√©n√©ration de {len(chunks)} fragments (chunks) optimis√©s.\")\n",
    "\n",
    "    # 4. Embeddings & Indexation\n",
    "    print(f\"üß† Calcul des vecteurs avec {MODEL_EMBEDDING}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=MODEL_EMBEDDING,\n",
    "        model_kwargs={'device': DEVICE}\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(DB_FAISS_PATH)\n",
    "    print(f\"‚úÖ Base de donn√©es sauvegard√©e avec succ√®s dans '{DB_FAISS_PATH}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_process_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd37489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nasser/Projects/Conversational-Chatbot/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‚öôÔ∏è CHARGEMENT DU SYST√àME M√âMOIRE (CUDA) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23069/3008227541.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base charg√©e.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"Groq_API_KEY\")\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n",
    "MODEL_EMBEDDING = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "MODEL_LLM = \"llama-3.3-70b-versatile\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"--- ‚öôÔ∏è CHARGEMENT DU SYST√àME M√âMOIRE ({DEVICE.upper()}) ---\")\n",
    "\n",
    "# 1. Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=MODEL_EMBEDDING,\n",
    "    model_kwargs={'device': DEVICE}\n",
    ")\n",
    "\n",
    "# 2. Vectorstore & Retriever\n",
    "try:\n",
    "    vectorstore = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={'k': 8})\n",
    "    print(\"‚úÖ Base charg√©e.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de chargement : {e}\")\n",
    "\n",
    "# 3. LLM\n",
    "llm = ChatGroq(temperature=0.0, model_name=MODEL_LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbd69ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Syst√®me de m√©moire conversationnelle pr√™t.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableBranch\n",
    "\n",
    "# --- √âTAPE 1 : LA CHA√éNE DE REFORMULATION (Contextualize) ---\n",
    "# Ce prompt sert uniquement √† r√©√©crire la question en fonction de l'historique\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question which might reference context in the chat history, \n",
    "formulate a standalone question which can be understood without the chat history. \n",
    "Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", contextualize_q_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Cette cha√Æne ne s'active que s'il y a un historique\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- √âTAPE 2 : LA CHA√éNE DE R√âPONSE (Answer) ---\n",
    "qa_system_prompt = \"\"\"\n",
    "You are an expert academic advisor for international students.\n",
    "You have access to official documents from several universities.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1.  **Analyze the Context:** Look at the \"Document Source\" header in each text chunk.\n",
    "2.  **Language:** ALWAYS answer in **ENGLISH**.\n",
    "3.  **Accuracy:** Use the provided context only. If you don't know, say so.\n",
    "4.  **Format:** Use bullet points for lists.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # On garde l'historique aussi ici au cas o√π\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# Fonction pour formater les docs\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# La cha√Æne finale\n",
    "qa_chain = qa_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úÖ Syst√®me de m√©moire conversationnelle pr√™t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adfb15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ MEMORY CHATBOT READY (English Output)\n",
      "Example: Ask about a university, then just ask 'What are the fees?'\n",
      "============================================================\n",
      "\n",
      "üîç Recherche dans les documents...\n",
      "üß† G√©n√©ration de la r√©ponse...\n",
      "\n",
      "------------------------------------------------------------\n",
      "‚è±Ô∏è 2.31s | Sources: HASSAN2_UNIVERCITY.pdf\n",
      "------------------------------------------------------------\n",
      "üí° ANSWER:\n",
      "It seems like you provided a large document in French and Arabic, which appears to be an activity report from the University Hassan II of Casablanca (UH2C) for the year 2019-2020. \n",
      "\n",
      "To provide a helpful response, I would like to know what specific information you are looking for in this document. Are you interested in:\n",
      "\n",
      "* International partnerships and cooperation?\n",
      "* Campus life and student activities?\n",
      "* Academic programs and research?\n",
      "* University governance and quality assurance?\n",
      "* Something else?\n",
      "\n",
      "Please let me know, and I'll do my best to provide a helpful and accurate response based on the provided document. \n",
      "\n",
      "Here are some general points that can be gathered from the document:\n",
      "* The university participated in various events, such as the \"Web Act for Impact\" webinar and the \"SIEL 2020\" literary competition.\n",
      "* The university has a strong focus on internationalization, with partnerships and cooperation with institutions from around the world.\n",
      "* The university is committed to quality assurance and has participated in events and workshops to improve its quality assurance processes.\n",
      "* The university has a range of academic programs and research activities, including programs in fields such as science, economics, and social sciences.\n",
      "\n",
      "If you have any specific questions or would like more information on any of these points, please let me know.\n",
      "============================================================\n",
      "\n",
      "üîÑ Reformulation de la question avec le contexte...\n",
      "   (Question interne : 'Here's some general information about Qatar University:\n",
      "\n",
      "**Overview**\n",
      "Qatar University (QU) is a public research university located in Doha, Qatar. It was founded in 1973 and is the oldest and largest university in the country.\n",
      "\n",
      "**Academics**\n",
      "QU offers a wide range of academic programs, including undergraduate and graduate degrees, as well as doctoral programs. The university is composed of nine colleges:\n",
      "\n",
      "1. College of Arts and Sciences\n",
      "2. College of Business and Economics\n",
      "3. College of Education\n",
      "4. College of Engineering\n",
      "5. College of Health Sciences\n",
      "6. College of Law\n",
      "7. College of Medicine\n",
      "8. College of Pharmacy\n",
      "9. College of Sharia and Islamic Studies\n",
      "\n",
      "**Research**\n",
      "QU is a research-intensive university, with a strong focus on interdisciplinary research and collaboration with international partners. The university has established research centers and institutes in areas such as energy, environment, and healthcare.\n",
      "\n",
      "**Campus**\n",
      "The university's campus is located in the northern part of Doha, and it covers an area of approximately 1.2 million square meters. The campus features modern facilities, including state-of-the-art classrooms, laboratories, and libraries.\n",
      "\n",
      "**Student Life**\n",
      "QU has a diverse student body, with over 20,000 students from more than 50 countries. The university offers a range of extracurricular activities, including sports, cultural events, and community service programs.\n",
      "\n",
      "**International Partnerships**\n",
      "QU has established partnerships with universities and institutions around the world, including the United States, Europe, and Asia. These partnerships provide opportunities for student exchange, research collaboration, and faculty development.\n",
      "\n",
      "**Accreditation**\n",
      "QU is accredited by the Ministry of Education and Higher Education in Qatar, and it is also accredited by international agencies such as the Accreditation Board for Engineering and Technology (ABET) and the Association to Advance Collegiate Schools of Business (AACSB).\n",
      "\n",
      "I hope this information helps! Let me know if you have any specific questions or if you'd like more information about Qatar University.')\n",
      "üîç Recherche dans les documents...\n",
      "üß† G√©n√©ration de la r√©ponse...\n",
      "\n",
      "------------------------------------------------------------\n",
      "‚è±Ô∏è 2.75s | Sources: Qatar_univercity.pdf\n",
      "------------------------------------------------------------\n",
      "üí° ANSWER:\n",
      "Thank you for providing the general information about Qatar University. Based on the text you provided earlier, I can see that it matches the information you provided here. \n",
      "\n",
      "To summarize, here are the key points about Qatar University:\n",
      "\n",
      "* **History and Overview**: Founded in 1973, Qatar University is the oldest and largest university in Qatar.\n",
      "* **Academics**: The university offers a wide range of academic programs, including undergraduate and graduate degrees, as well as doctoral programs, through its 9 colleges.\n",
      "* **Research**: Qatar University is a research-intensive university with a focus on interdisciplinary research and international collaboration.\n",
      "* **Campus**: The university's campus is located in the northern part of Doha and features modern facilities.\n",
      "* **Student Life**: The university has a diverse student body with over 20,000 students from more than 50 countries and offers various extracurricular activities.\n",
      "* **International Partnerships**: Qatar University has established partnerships with universities and institutions around the world.\n",
      "* **Accreditation**: The university is accredited by the Ministry of Education and Higher Education in Qatar and international agencies such as ABET and AACSB.\n",
      "\n",
      "If you have any specific questions or would like more information about Qatar University, feel free to ask. Some potential questions could be:\n",
      "* What are the admission requirements for international students?\n",
      "* What kind of support services does the university offer for international students?\n",
      "* Can you provide more information about the research centers and institutes at Qatar University?\n",
      "* What are the university's strengths and weaknesses in terms of academic programs and research?\n",
      "\n",
      "Let me know if you have any specific questions or if there's anything else I can help you with.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import time\n",
    "\n",
    "def start_memory_chat():\n",
    "    # 1. Initialisation de la m√©moire pour cette session\n",
    "    chat_history = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ü§ñ MEMORY CHATBOT READY (English Output)\")\n",
    "    print(\"Example: Ask about a university, then just ask 'What are the fees?'\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    while True:\n",
    "        question = input(\"üëâ VOUS : \")\n",
    "        if question.lower() in ['exit', 'quit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # --- PHASE A : REFORMULATION DE LA QUESTION ---\n",
    "            if chat_history:\n",
    "                # Si on a de la m√©moire, on demande au LLM de pr√©ciser la question\n",
    "                print(\"üîÑ Reformulation de la question avec le contexte...\")\n",
    "                reformulated_question = contextualize_q_chain.invoke({\n",
    "                    \"chat_history\": chat_history,\n",
    "                    \"question\": question\n",
    "                })\n",
    "                print(f\"   (Question interne : '{reformulated_question}')\")\n",
    "            else:\n",
    "                reformulated_question = question\n",
    "\n",
    "            # --- PHASE B : RECHERCHE (Retrieval) ---\n",
    "            # On cherche avec la question REFORMUL√âE (plus pr√©cise)\n",
    "            print(\"üîç Recherche dans les documents...\")\n",
    "            retrieved_docs = retriever.invoke(reformulated_question)\n",
    "            \n",
    "            # --- PHASE C : G√âN√âRATION ---\n",
    "            print(\"üß† G√©n√©ration de la r√©ponse...\")\n",
    "            response = qa_chain.invoke({\n",
    "                \"context\": format_docs(retrieved_docs),\n",
    "                \"chat_history\": chat_history,\n",
    "                \"question\": reformulated_question\n",
    "            })\n",
    "            \n",
    "            # --- PHASE D : MISE √Ä JOUR M√âMOIRE ---\n",
    "            # On stocke l'√©change pour le prochain tour\n",
    "            chat_history.extend([\n",
    "                HumanMessage(content=question),\n",
    "                AIMessage(content=response)\n",
    "            ])\n",
    "            \n",
    "            # Affichage\n",
    "            end_time = time.time()\n",
    "            sources = set([d.metadata.get('source', '').split('/')[-1] for d in retrieved_docs])\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(f\"‚è±Ô∏è {end_time - start_time:.2f}s | Sources: {', '.join(sources)}\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"üí° ANSWER:\\n{response}\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_memory_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
